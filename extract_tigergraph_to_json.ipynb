# In the Google Cloud platform, navigate to Vertex AI Workbench to managed notebooks and create a new notebook. Launch the new notebook.

# you'll want to select either PySpark local kernel or Serverless Spark. install required dependencies:
!pip install pyTigerGraph --user --quiet
!pip install pyspark[sql] --user --quiet

# import libraries
import pyTigerGraph as tg
import json
import pandas as pd
import getpass

# specify credentials for your tigergraph instance
host = "https://ce.i.tgcloud.io"
graphname = "social"
username = [your username]
version = "3.0.5" #@param ["3.0.5", "3.0.0", "2.6.2"] {allow-input: true}
useCert = True #@param {type:"boolean"}

# enter the password and secrets
password = getpass.getpass()
secret = getpass.getpass()

# create the connection
conn = tg.TigerGraphConnection(host=host, graphname=graphname, username=username, password=password, gsqlSecret=secret)
token = conn.getToken(secret, setToken=True)
print(token)

# now get some data
def getLoadedStats(limit=5):
  numPeople = conn.getVertexCount("person")

  people = conn.getVertices("person", limit=limit)

  print(f"The re are currently {numPeople} people, edges connecting them")
  print(f"Our people are: {json.dumps(people, indent=2)}")

getLoadedStats()

# your results may be different depending on what you are trying to do
results = conn.getVerticesById("person", "Tom")
print(json.dumps(results, indent=2))

# now use pyspark to read the data into a spark dataframe
# Import the required classes and functions
from pyspark.sql import SparkSession, Row

# Initialize a SparkSession
spark = SparkSession.builder.appName("tigergraph-solution").getOrCreate()

# Convert the query result to a list of Row objects
rows = [Row(**x) for x in results]

# Create a DataFrame from the list of Row objects
df = spark.createDataFrame(rows)

# Print the schema of the DataFrame
df.printSchema()

print(df)

# you can also check the schema this way to check the shape of the data
print(df.schema)

# and look at the dataframe
df.select("*").show()


